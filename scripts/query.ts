// query-supabase.ts
import path from "path";
import { fileURLToPath } from "node:url";
import { dirname } from "node:path";
import dotenv from "dotenv";
import { getEmbedding, setProxy, sleep } from "../lib/utils.ts";
import { createClient } from "@supabase/supabase-js";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";
import { QUERY_LIST } from "../lib/constant.ts";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
dotenv.config({
  path: path.resolve(__dirname, "../.env"),
  override: false,
  quiet: true,
});

const HF_API_KEY = process.env.HF_API_KEY || "";
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || "";
const OPENAI_API_BASE = process.env.OPENAI_API_BASE || "";

// ğŸ‘‡ ä½¿ç”¨ service_role_keyï¼ˆé«˜æƒé™ï¼Œå¯è°ƒç”¨ RPCï¼‰
const SUPABASE_URL = process.env.SUPABASE_URL!;
const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY!;

if (!SUPABASE_SERVICE_ROLE_KEY) {
  throw new Error("Missing SUPABASE_SERVICE_ROLE_KEY");
}

const EMBEDDING_MODEL = "BAAI/bge-m3";
const TOP_K = 5;

async function main() {
  await setProxy();
  // ğŸ‘‡ åˆ›å»º Supabase å®¢æˆ·ç«¯ï¼ˆæœåŠ¡ç«¯æ¨¡å¼ï¼‰
  const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);

  for (const QUERY_TEXT of QUERY_LIST) {
    console.log(`\nğŸ” æŸ¥è¯¢: ${QUERY_TEXT}`);

    // 1. è·å– query embedding
    const queryEmbedding = await getEmbedding(
      QUERY_TEXT,
      HF_API_KEY,
      EMBEDDING_MODEL
    );
    let embeddingArray: number[] = [];
    if (Array.isArray(queryEmbedding)) {
      embeddingArray = Array.isArray(queryEmbedding[0])
        ? queryEmbedding[0]
        : queryEmbedding;
    } else {
      embeddingArray = [queryEmbedding];
    }

    // 2. è°ƒç”¨ Supabase RPC å‡½æ•°è¿›è¡Œå‘é‡æ£€ç´¢
    const { data, error } = await supabase
      .rpc("match_chunks", {
        query_embedding: embeddingArray,
        match_count: TOP_K,
      })
      .select("chunk, meta, cosine_distance");

    if (error) {
      console.error("å‘é‡æ£€ç´¢å¤±è´¥:", error);
      continue;
    }

    const topChunks = data as {
      chunk: string;
      meta: unknown;
      cosine_distance: number;
    }[];
    const context = topChunks
      .map((c, i) => `ã€ç‰‡æ®µ${i + 1}ã€‘${c.chunk}`)
      .join("\n\n");

    // 3. è°ƒç”¨å¤§æ¨¡å‹
    const llm = new ChatOpenAI({
      openAIApiKey: OPENAI_API_KEY,
      configuration: { baseURL: OPENAI_API_BASE },
      modelName: "qwen-turbo",
      temperature: 0.2,
    });

    const systemPrompt = `ä½ æ˜¯ã€Šå¤©å¬è®¡åˆ’ï¼šç½—æ–¯é™·é˜±ã€‹å°è¯´é—®ç­”åŠ©æ‰‹ï¼Œè¯·ç»“åˆç»™å®šç‰‡æ®µå’Œç”¨æˆ·é—®é¢˜ï¼Œç²¾å‡†ã€ç®€æ´åœ°å›ç­”ã€‚ä¼˜å…ˆç”¨ç‰‡æ®µå†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ã€‚`;
    const messages = [
      new SystemMessage(systemPrompt),
      new HumanMessage(`å·²æ£€ç´¢ç‰‡æ®µï¼š\n${context}\n\nç”¨æˆ·é—®é¢˜ï¼š${QUERY_TEXT}`),
    ];

    const res = await llm.invoke(messages);
    console.log("ğŸ¤– AIå›ç­”ï¼š", res.content);
    console.log("--------------------------------------------------------");

    await sleep(3000);
  }
}

main().catch(console.error);
